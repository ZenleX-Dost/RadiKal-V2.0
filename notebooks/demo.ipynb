{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d798f20d",
   "metadata": {},
   "source": [
    "# XAI Visual Quality Control - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the complete functionality of the XAI Visual Quality Control system for radiographic defect detection.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. Image loading and preprocessing\n",
    "2. Defect detection with Faster R-CNN\n",
    "3. All 4 XAI explanation methods (Grad-CAM, SHAP, LIME, Integrated Gradients)\n",
    "4. Explanation aggregation and consensus scoring\n",
    "5. Uncertainty quantification with MC-Dropout\n",
    "6. Model calibration (ECE, Temperature Scaling)\n",
    "7. Comprehensive metrics calculation\n",
    "8. Visualization of results\n",
    "\n",
    "**Note**: Make sure you have trained a model first using `backend/scripts/train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df839401",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ade99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../backend')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Core modules\n",
    "from core.models.detector import DefectDetector\n",
    "from core.preprocessing.image_processor import ImageProcessor\n",
    "\n",
    "# XAI modules\n",
    "from core.xai.gradcam import GradCAM\n",
    "from core.xai.shap_explainer import SHAPExplainer\n",
    "from core.xai.lime_explainer import LIMEExplainer\n",
    "from core.xai.integrated_gradients import IntegratedGradientsExplainer\n",
    "from core.xai.aggregator import XAIAggregator\n",
    "\n",
    "# Uncertainty modules\n",
    "from core.uncertainty.mc_dropout import MCDropoutEstimator\n",
    "from core.uncertainty.calibration import calculate_ece, TemperatureScaling\n",
    "\n",
    "# Metrics modules\n",
    "from core.metrics.business_metrics import calculate_confusion_matrix_metrics\n",
    "from core.metrics.detection_metrics import calculate_map, calculate_auroc\n",
    "from core.metrics.segmentation_metrics import calculate_mean_iou\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdd845",
   "metadata": {},
   "source": [
    "## 2. Load Model and Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_PATH = '../backend/models/checkpoints/best_model.pth'\n",
    "\n",
    "# Initialize defect detector\n",
    "print(f\"Loading model on {DEVICE}...\")\n",
    "model = DefectDetector(num_classes=2, device=DEVICE)\n",
    "\n",
    "if Path(MODEL_PATH).exists():\n",
    "    model.load_weights(MODEL_PATH)\n",
    "    print(f\"Model loaded from {MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"Warning: Model weights not found. Using untrained model for demonstration.\")\n",
    "\n",
    "# Initialize image processor\n",
    "image_processor = ImageProcessor(target_size=(512, 512))\n",
    "\n",
    "# Initialize XAI explainers\n",
    "print(\"Initializing XAI explainers...\")\n",
    "gradcam = GradCAM(model.model)\n",
    "shap_explainer = SHAPExplainer(model.model)\n",
    "lime_explainer = LIMEExplainer(model.model)\n",
    "ig_explainer = IntegratedGradientsExplainer(model.model)\n",
    "xai_aggregator = XAIAggregator()\n",
    "\n",
    "# Initialize uncertainty estimator\n",
    "mc_dropout = MCDropoutEstimator(model.model, n_samples=10, device=DEVICE)\n",
    "\n",
    "print(\"All components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caa702",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f300d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image (use a sample from your test dataset)\n",
    "test_image_path = '../backend/data/test/images/test_0000.jpg'\n",
    "\n",
    "if not Path(test_image_path).exists():\n",
    "    print(f\"Test image not found at {test_image_path}\")\n",
    "    print(\"Please run: python backend/scripts/generate_test_dataset.py\")\n",
    "    print(\"Or provide your own test image\")\n",
    "else:\n",
    "    # Load image\n",
    "    image = image_processor.load_image(test_image_path)\n",
    "    print(f\"Image loaded: shape={image.shape}, dtype={image.dtype}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    preprocessed = image_processor.preprocess(image)\n",
    "    print(f\"Preprocessed: shape={preprocessed.shape}, dtype={preprocessed.dtype}\")\n",
    "    \n",
    "    # Convert to tensor\n",
    "    image_tensor = torch.from_numpy(\n",
    "        image_processor.to_tensor(preprocessed)\n",
    "    ).float().unsqueeze(0).to(DEVICE)\n",
    "    print(f\"Tensor shape: {image_tensor.shape}\")\n",
    "    \n",
    "    # Visualize original image\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "    plt.title('Original Test Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838be8b",
   "metadata": {},
   "source": [
    "## 4. Run Defect Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection\n",
    "detections = model.predict(image_tensor)\n",
    "\n",
    "print(f\"Number of detections: {len(detections)}\")\n",
    "print(\"\\nDetection results:\")\n",
    "for i, det in enumerate(detections):\n",
    "    print(f\"  Detection {i+1}:\")\n",
    "    print(f\"    - Label: {det['label']}\")\n",
    "    print(f\"    - Confidence: {det['score']:.3f}\")\n",
    "    print(f\"    - Bounding Box: {det['box']}\")\n",
    "    \n",
    "# Visualize detections\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "\n",
    "for det in detections:\n",
    "    box = det['box']\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    # Draw bounding box\n",
    "    rect = plt.Rectangle((x1, y1), width, height, \n",
    "                         fill=False, edgecolor='red', linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "    \n",
    "    # Add label\n",
    "    label = f\"{det['label']} ({det['score']:.2f})\"\n",
    "    plt.text(x1, y1-5, label, color='red', fontsize=10, \n",
    "            bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.title(f'Defect Detection Results ({len(detections)} detections)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17120a5",
   "metadata": {},
   "source": [
    "## 5. Generate XAI Explanations\n",
    "\n",
    "Now let's generate explanations using all 4 XAI methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607150de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanations from all 4 methods\n",
    "explanations = {}\n",
    "\n",
    "print(\"Generating XAI explanations...\")\n",
    "\n",
    "# Grad-CAM\n",
    "print(\"  1/4 Grad-CAM...\")\n",
    "explanations['gradcam'] = gradcam.generate_heatmap(image_tensor, target_class=1)\n",
    "\n",
    "# SHAP\n",
    "print(\"  2/4 SHAP...\")\n",
    "explanations['shap'] = shap_explainer.generate_heatmap(image_tensor, target_class=1)\n",
    "\n",
    "# LIME\n",
    "print(\"  3/4 LIME...\")\n",
    "explanations['lime'] = lime_explainer.generate_heatmap(image_tensor, target_class=1)\n",
    "\n",
    "# Integrated Gradients\n",
    "print(\"  4/4 Integrated Gradients...\")\n",
    "explanations['ig'] = ig_explainer.generate_heatmap(image_tensor, target_class=1, baseline='black')\n",
    "\n",
    "print(\"All explanations generated!\")\n",
    "\n",
    "# Visualize all explanations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "axes[0, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Grad-CAM\n",
    "axes[0, 1].imshow(explanations['gradcam'], cmap='jet')\n",
    "axes[0, 1].set_title('Grad-CAM', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# SHAP\n",
    "axes[0, 2].imshow(explanations['shap'], cmap='jet')\n",
    "axes[0, 2].set_title('SHAP', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# LIME\n",
    "axes[1, 0].imshow(explanations['lime'], cmap='jet')\n",
    "axes[1, 0].set_title('LIME', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Integrated Gradients\n",
    "axes[1, 1].imshow(explanations['ig'], cmap='jet')\n",
    "axes[1, 1].set_title('Integrated Gradients', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Leave last subplot empty for now\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('XAI Explanations Comparison', fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309f1cd",
   "metadata": {},
   "source": [
    "## 6. Aggregate Explanations and Compute Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all explanations\n",
    "heatmaps_list = list(explanations.values())\n",
    "\n",
    "# Try different aggregation methods\n",
    "aggregation_methods = ['mean', 'median', 'weighted']\n",
    "aggregated_results = {}\n",
    "\n",
    "for method in aggregation_methods:\n",
    "    if method == 'weighted':\n",
    "        weights = [0.3, 0.3, 0.2, 0.2]  # Prioritize Grad-CAM and SHAP\n",
    "        aggregated = xai_aggregator.aggregate(heatmaps_list, method=method, weights=weights)\n",
    "    else:\n",
    "        aggregated = xai_aggregator.aggregate(heatmaps_list, method=method)\n",
    "    \n",
    "    aggregated_results[method] = aggregated\n",
    "\n",
    "# Compute consensus score\n",
    "consensus_metrics = ['correlation', 'iou', 'dice']\n",
    "consensus_scores = {}\n",
    "\n",
    "for metric in consensus_metrics:\n",
    "    score = xai_aggregator.compute_consensus_score(heatmaps_list, metric=metric)\n",
    "    consensus_scores[metric] = score\n",
    "    print(f\"Consensus Score ({metric}): {score:.4f}\")\n",
    "\n",
    "# Visualize aggregated results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx, (method, heatmap) in enumerate(aggregated_results.items(), 1):\n",
    "    axes[idx].imshow(heatmap, cmap='jet')\n",
    "    axes[idx].set_title(f'Aggregated ({method})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Aggregated Explanations | Consensus: {consensus_scores[\"correlation\"]:.3f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a64d1",
   "metadata": {},
   "source": [
    "## 7. Uncertainty Quantification with MC-Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13128b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictive uncertainty using MC-Dropout\n",
    "print(\"Computing uncertainty with MC-Dropout (10 samples)...\")\n",
    "\n",
    "# Get uncertainty map\n",
    "uncertainty_map = mc_dropout.compute_predictive_entropy(image_tensor)\n",
    "mean_uncertainty = uncertainty_map.mean()\n",
    "max_uncertainty = uncertainty_map.max()\n",
    "\n",
    "print(f\"Mean Uncertainty: {mean_uncertainty:.4f}\")\n",
    "print(f\"Max Uncertainty: {max_uncertainty:.4f}\")\n",
    "\n",
    "# Visualize uncertainty\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Uncertainty map\n",
    "im1 = axes[1].imshow(uncertainty_map, cmap='hot')\n",
    "axes[1].set_title('Predictive Entropy (Uncertainty)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "# Overlay uncertainty on image\n",
    "axes[2].imshow(image, cmap='gray' if len(image.shape) == 2 else None, alpha=0.7)\n",
    "im2 = axes[2].imshow(uncertainty_map, cmap='hot', alpha=0.3)\n",
    "axes[2].set_title('Uncertainty Overlay', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f'MC-Dropout Uncertainty | Mean: {mean_uncertainty:.4f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12666123",
   "metadata": {},
   "source": [
    "## 8. Model Calibration Metrics\n",
    "\n",
    "Check the model's calibration using Expected Calibration Error (ECE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's simulate some predictions and compute ECE\n",
    "# In practice, you would evaluate on your validation set\n",
    "\n",
    "# Simulate predictions (replace with actual validation data)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "confidences = np.random.beta(5, 2, n_samples)  # Simulated confidence scores\n",
    "predictions = (confidences > 0.5).astype(int)\n",
    "labels = np.random.randint(0, 2, n_samples)  # Simulated ground truth\n",
    "\n",
    "# Calculate ECE\n",
    "ece = calculate_ece(\n",
    "    torch.tensor(confidences),\n",
    "    torch.tensor(predictions),\n",
    "    torch.tensor(labels),\n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "print(f\"Interpretation: {'Well-calibrated' if ece < 0.1 else 'Needs calibration'}\")\n",
    "\n",
    "# Visualize calibration\n",
    "from core.uncertainty.calibration import plot_reliability_diagram\n",
    "\n",
    "fig = plot_reliability_diagram(\n",
    "    torch.tensor(confidences),\n",
    "    torch.tensor(predictions),\n",
    "    torch.tensor(labels),\n",
    "    n_bins=10\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: This is simulated data. Run calibration on actual validation set for real metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7355c31",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. ✅ **Model Loading**: Loaded Faster R-CNN defect detector\n",
    "2. ✅ **Image Processing**: Preprocessed radiographic images\n",
    "3. ✅ **Defect Detection**: Detected and visualized defects with bounding boxes\n",
    "4. ✅ **XAI Explanations**: Generated heatmaps using 4 methods (Grad-CAM, SHAP, LIME, IG)\n",
    "5. ✅ **Aggregation**: Combined explanations and computed consensus scores\n",
    "6. ✅ **Uncertainty**: Quantified prediction uncertainty with MC-Dropout\n",
    "7. ✅ **Calibration**: Calculated Expected Calibration Error (ECE)\n",
    "\n",
    "### Key Results:\n",
    "- Detection confidence scores show model reliability\n",
    "- XAI heatmaps highlight relevant image regions\n",
    "- Consensus scores indicate agreement between methods\n",
    "- Uncertainty maps identify ambiguous areas\n",
    "- ECE shows calibration quality\n",
    "\n",
    "### Next Steps:\n",
    "1. **Train on Real Data**: Replace synthetic data with actual radiographic images\n",
    "2. **API Integration**: Use the FastAPI endpoints (`/api/xai-qc/detect`, `/explain`)\n",
    "3. **Frontend Development**: Build the Makerkit UI to consume these APIs\n",
    "4. **Production Deployment**: Dockerize and deploy with `docker-compose.yml`\n",
    "5. **Continuous Monitoring**: Track metrics over time with MLflow\n",
    "\n",
    "For API usage, see: `backend/api/routes.py`  \n",
    "For training: `backend/scripts/train.py`  \n",
    "For deployment: `docker-compose.yml` (to be created)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
